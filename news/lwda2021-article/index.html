<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="en"><meta name=author content="Ioannis Prapas"><meta name=description content="Check my research article presentation on &ldquo;Continuous Training and Deployment of Deep Learning models&rdquo; at LWDA 2021 - Wednesday, Sep 1st 14:00-14:20 CET. It is free to attend, you only need to register here.
This article is based on the thesis I did for the completion of my studies in the Big Data Management and Analytics (BDMA) Erasmus Mundus MSc.
Abstract
Deep Learning (DL) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases."><meta name=keywords content="blog,big data,machine learning,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Work on Continuous Deployment of Deep Learning models presented at LWDA 2021"><meta name=twitter:description content="Check my research article presentation on &ldquo;Continuous Training and Deployment of Deep Learning models&rdquo; at LWDA 2021 - Wednesday, Sep 1st 14:00-14:20 CET. It is free to attend, you only need to register here.
This article is based on the thesis I did for the completion of my studies in the Big Data Management and Analytics (BDMA) Erasmus Mundus MSc.
Abstract
Deep Learning (DL) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases."><meta property="og:title" content="Work on Continuous Deployment of Deep Learning models presented at LWDA 2021"><meta property="og:description" content="Check my research article presentation on &ldquo;Continuous Training and Deployment of Deep Learning models&rdquo; at LWDA 2021 - Wednesday, Sep 1st 14:00-14:20 CET. It is free to attend, you only need to register here.
This article is based on the thesis I did for the completion of my studies in the Big Data Management and Analytics (BDMA) Erasmus Mundus MSc.
Abstract
Deep Learning (DL) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases."><meta property="og:type" content="article"><meta property="og:url" content="https://iprapas.github.io/news/lwda2021-article/"><meta property="article:section" content="news"><meta property="article:published_time" content="2021-08-30T00:00:00+00:00"><meta property="article:modified_time" content="2021-08-30T00:00:00+00:00"><base href=https://iprapas.github.io/news/lwda2021-article/><title>Work on Continuous Deployment of Deep Learning models presented at LWDA 2021 · Ioannis Prapas</title><link rel=canonical href=https://iprapas.github.io/news/lwda2021-article/><link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel=stylesheet><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.13.0/css/all.css integrity=sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin=anonymous><link rel=stylesheet href=../../css/coder.min.a4f332213a21ce8eb521670c614470c58923aaaf385e2a73982c31dd7642decb.css integrity="sha256-pPMyITohzo61IWcMYURwxYkjqq84XipzmCwx3XZC3ss=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=https://iprapas.github.io/images/favicon.png sizes=32x32><link rel=icon type=image/png href=https://iprapas.github.io/images/favicon.png sizes=16x16><meta name=generator content="Hugo 0.119.0"></head><body class=colorscheme-light><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=../../>Ioannis Prapas</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fas fa-bars"></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=https://iprapas.github.io/>Home</a></li><li class=navigation-item><a class=navigation-link href=https://iprapas.github.io/news/>News</a></li><li class=navigation-item><a class=navigation-link href=https://iprapas.github.io/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=https://iprapas.github.io/research/>Research</a></li><li class=navigation-item><a class=navigation-link href=https://iprapas.github.io/about/>About</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1>Work on Continuous Deployment of Deep Learning models presented at LWDA 2021</h1></header><p>Check my research article presentation on &ldquo;Continuous Training and Deployment of Deep Learning models&rdquo; at LWDA 2021 - Wednesday, Sep 1st 14:00-14:20 CET. It is free to attend, you only need to register <a href=https://mcml.ai/lwda2021/attending>here</a>.</p><p>This article is based on the thesis I did for the completion of my studies in the Big Data Management and Analytics (BDMA) Erasmus Mundus MSc.</p><p><em>Abstract</em></p><blockquote><p>Deep Learning (DL) has consistently surpassed other Machine Learning methods and achieved state-of-the-art performance in multiple cases. Several modern applications like financial and recommender systems require models that are constantly updated with fresh data. The prominent approach for keeping a DL model fresh is to trigger full retraining from scratch when enough new data are available. However, retraining large and complex DL models is time-consuming and compute-intensive. This makes full retraining costly, wasteful, and slow. In this paper, we present an approach to continuously train and deploy DL models. First, we enable continuous training through proactive training that combines samples of historical data with new streaming data. Second, we enable continuous deployment through gradient sparsification that allows us to send a small percentage of the model updates per training iteration. Our experimental results with LeNet5 on MNIST and modern DL models on CIFAR-10 show that proactive training keeps models fresh with comparable - if not superior - performance to full retraining at a fraction of the time. Combined with gradient sparsification, sparse proactive training enables very fast updates of a deployed model with arbitrarily large sparsity, reducing communication per iteration up to four orders of magnitude, with minimal - if any - losses in model quality. Sparse training, however, comes at a price; it incurs overhead on the training that depends on the size of the model and increases the training time by factors ranging from 1.25 to 3 in our experiments. Arguably, a small price to pay for successfully enabling the continuous training and deployment of large DL models.</p></blockquote><p>(<a href=../../blog/lwda2021-article/dl_continuous_deployment.pdf>preprint pdf</a> | <a href=https://github.com/iprapas/dl-continuous-deployment>code</a>)</p></article></section><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type=text/javascript async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/startup.js id=MathJax-script></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}}</script></div><footer class=footer><section class=container><p>Subscribe to the <a href=https://forms.gle/kjeq3ajUQaJ4kemu7>newsletter</a> for a new post every now and then.</p>©
2023
Ioannis Prapas
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-93894885-1","auto"),ga("send","pageview"))</script></body></html>